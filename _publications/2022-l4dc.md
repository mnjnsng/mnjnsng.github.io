---
title: "L1MBRL"
collection: publications
permalink: /publication/12/1/2022-l4dc
excerpt: "The current paper studies a protective mission to defend a domain called the safe zone from a rogue drone invasion."
date: 12/1/2022
venue: "L4DC 2023 (under review)"
---

[Download paper here](https://drive.google.com/file/d/1-6-3y2cJb81-r10iXbmeD7VGszbqjyE3/view?usp=share_link)

[View code here](https://github.com/mnjnsng/L1-MBRL)

## Abstract

We present $\mathcal{L}_1$ - MBRL , a control-theoretic add-on scheme for model based reinforcement learning (MBRL) algorithms. Model based RL algorithms differ from model-free approaches in that they use data to learn a model of the transition dynamics, using which a control input is designed. Our add-on scheme utilizes the transition dynamics model to design a control input augmentation which perturbs the input produced by the MBRL algorithm. The add-on scheme relies on $\mathcal{L}_1$ - adaptive control to perturb the MBRL input to counter the effects due to epistemic uncertainties in the learned model of the transition dynamics. Epistemic uncertainties are always present due to a lack of sufficient data available to the high-level MBRL algorithms. Thus, we design the add-on scheme to make the MBRL algorithm robust to (unlearned) uncertainties in the transition dynamics. Notably, the add-on scheme does not put any additional requirements on the high-level MBRL. Therefore, the MBRL algorithm can operate as it is designed to and is entirely agnostic to the presence of the add-on scheme. We demonstrate the efficacy of our methodology using numerical simulations on several platforms common for testing RL algorithms. Furthermore, since our add-on scheme utilizes $\mathcal{L}_1$ - adaptive control, a robust adaptive control methodology, we also observe superior performance and sample efficiency in the presence of observation and action noise which most real systems are subject to.

## Main Results

We follow the $\mathcal{L}_1$ - MBRL framework that follows the following algorithm. Please refer to our full [paper](https://drive.google.com/file/d/1-6-3y2cJb81-r10iXbmeD7VGszbqjyE3/view?usp=share_link) for notations.

![](/images/publication/L1MBRL/algorithm.PNG)

The point here is that we make control-affine approximation of the nonlinear model $\hat{f}_\theta$ learned from the baseline MBRL algorithm along the trajectory at local affinization point $\bar{u}$. This is a more refined method than conventional method to simply enforce the neural network structure to be affine in control[^fn1].

We have conducted an experiment in five different Mujoco environments, while using METRPO [^fn2] as the baseline. Moreover, we injected action or observation noise in the system to also compare the robustness of the $\mathcal{L}_1$ augmentation to aleatoric uncertainty. Following table summarizes the results. The performance is averaged across four random seeds with a window size of 3000 timesteps at the end of the training. Higher performance is written in bold with green background. Noticeably, the $\mathcal{L}_1$ augmentation has significantly improved the performance of the baseline algorithm in almost all cases.

![](/images/publication/L1MBRL/Table.PNG)

Below are the learning curves as a function of episodic steps. The performance is averaged across four random seeds such that the solid lines indicate the average return of the four seeds at the corresponding timestep, and the colored planes indicate one-standard deviation.

![](/images/publication/L1MBRL/Results.png)
[^fn1]: Khojasteh, Mohammad Javad, et al. "Probabilistic safety constraints for learned high relative degree system dynamics." Learning for Dynamics and Control. PMLR, 2020.
[^fn2]: Kurutach, Thanard, et al. "Model-Ensemble Trust-Region Policy Optimization." International Conference on Learning Representations. 2018.
